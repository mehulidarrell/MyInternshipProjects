# -*- coding: utf-8 -*-
"""imgclfmnist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10JssduD1kCrGs3M3eBFj2eqIDGvHYknv
"""

# Import dependencies
import torch
from PIL import Image
from torch import nn, save, load
from torch.optim import Adam
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision.transforms import ToTensor

#get data

train= datasets.MNIST(root="/kaggle/working/mnist-dataset", train= True, download=True, transform=ToTensor())
dataset= DataLoader(train ,32) #batches of 32 images
#1, 28, 28, classes: 0-

#Image Classifier Neural Network

class ImageClassifier(nn.Module):
    def __init__(self):
        super().__init__()
        #basically stack all our layers
        self.model= nn.Sequential(
            nn.Conv2d(1, 32, (3,3)),
            nn.ReLU(),
            nn.Conv2d(32, 64, (3,3)),
            nn.ReLU(),
            nn.Conv2d(64, 64, (3,3)),
            nn.ReLU(),
            nn.Flatten(),
            nn.Linear(64*(28-6)*(28-6), 10)
        )
    def forward(self,x):
        return self.model(x)

#Instance of the Neural Network, loss, optimizer

clf= ImageClassifier().to("cuda")
opt= Adam(clf.parameters(), lr= 1e-3)
loss_fn= nn.CrossEntropyLoss()

#Training Flow

if __name__=="__main__":
    for epoch in range(10): #training for 10 epochs
        for batch in dataset:
            X,y= batch
            X,y= X.to("cuda"), y.to("cuda")
            yhat= clf(X)
            loss= loss_fn(yhat, y)

            #Apply backpropagation
            opt.zero_grad()
            loss.backward()
            opt.step()

        print(f"Epoch:{epoch} loss is {loss.item()}")

    with open('model_state.pt','wb') as f:
        save(clf.state_dict(), f)